{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58783594-63e8-4543-9cac-e630bfd7057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"evaluation_questions.csv\")\n",
    "df\n",
    "\n",
    "from vectordb_utils_shule import ShuleVectorDB\n",
    "from prompt_utils import build_prompt\n",
    "import pickle\n",
    "import time\n",
    "from openai_utils import get_completion_openai, init_openai\n",
    "from logger import log_info, log_warning\n",
    "from datasets import Dataset\n",
    "\n",
    "index_path = \"../index/index_n179366_03171945.pickle\"\n",
    "top_n = 8\n",
    "recall_n = 80\n",
    "search_strategy = \"hnsw\"\n",
    "source = \"Hybrid\"\n",
    "model= \"GPT-3.5\"\n",
    "\n",
    "with open(index_path, 'rb') as file:\n",
    "        vec_db_shule = pickle.load(file)\n",
    "    \n",
    "def search_db(user_input, source_type):\n",
    "    search_results = []\n",
    "    \n",
    "    if search_strategy == \"hnsw\":\n",
    "        search_labels = vec_db_shule.search_bge(user_input, top_n)\n",
    "        texts, pages, titles, years, countries, ORGs = vec_db_shule.get_context_by_labels(search_labels)\n",
    "        search_results = texts\n",
    "        res = [texts[i] if countries[i] == 'xxx' else f'In {countries[i]}, {texts[i]}' for i in range(top_n)]\n",
    "\n",
    "        search_field = \"\\n\\n\".join([f\"{i+1}. [Reference: {titles[i]}, Page: {pages[i]}, ORG: {ORGs[i]}, Year: {years[i]}]\\n{texts[i]}\" for i in range(top_n)])\n",
    "        prompt = build_prompt(source_type=source_type, info=[f\"{res[i]} [Reference: Page {pages[i]}, {titles[i]}, {years[i]}, {ORGs[i]}]\" for i in range(top_n)], query=user_input)\n",
    "        \n",
    "    elif search_strategy == \"rerank\":\n",
    "        scores, texts, pages, titles, years, countries, ORGs = rerank(user_input, top_n, recall_n)\n",
    "        search_results = texts\n",
    "        res = [texts[i] if countries[i] == 'xxx' else f'In {countries[i]}, {texts[i]}' for i in range(top_n)]\n",
    "\n",
    "        search_field = \"\\n\\n\".join([f\"{i+1}. [Reference: {titles[i]}, Page: {pages[i]}, ORG: {ORGs[i]}, Year: {years[i]}]\\n{texts[i]}\" for i in range(top_n)])\n",
    "        prompt = build_prompt(source_type=source_type, info=[f\"{res[i]} [Reference: Page {pages[i]}, {titles[i]}, {years[i]}, {ORGs[i]}]\" for i in range(top_n)], query=user_input)\n",
    "        \n",
    "    elif search_strategy == \"fusion\":\n",
    "        log_warning(\"Not support yet.\")\n",
    "        return\n",
    "            \n",
    "    log_info(f\"prompt content built:\\n{prompt}\")\n",
    "    return prompt, search_results\n",
    "\n",
    "\n",
    "def rerank(user_input, top_n, recall_n):\n",
    "    search_labels = vec_db_shule.search_bge(user_input, recall_n)\n",
    "    t0 = time.time()\n",
    "    texts, pages, titles, years, countries, ORGs = vec_db_shule.get_context_by_labels(search_labels)\n",
    "    t1 = time.time()\n",
    "    log_info(f\"vec_db_shule.get_context_by_labels costs: {t1 - t0}\")\n",
    "\n",
    "    documents = [texts[i] if countries[i] == 'xxx' else f'In {countries[i]}, {texts[i]}' for i in range(len(pages))]\n",
    "    res = rerank_model.rank(documents = documents,\n",
    "                            query=user_input,\n",
    "                            batch_size = 1,\n",
    "                            return_documents = False,\n",
    "                            show_progress_bar = False)\n",
    "    t2 = time.time()\n",
    "    log_info(f\"rerank_model.predict costs: {t2 - t1}\")\n",
    "    \n",
    "    ids = [i['corpus_id'] for i in res][:top_n]\n",
    "    scores = [i['score'] for i in res][:top_n]\n",
    "\n",
    "    log_info(f\"finish rerank {recall_n} texts, return highest {top_n} texts\")\n",
    "    return scores, [texts[i] for i in ids], [pages[i] for i in ids], [titles[i] for i in ids], [years[i] for i in ids], [countries[i] for i in ids], [ORGs[i] for i in ids]\n",
    "\n",
    "questions = df[\"Questions\"].to_list()\n",
    "ground_truths = df[\"Groud_answer\"].to_list()\n",
    "answers = []\n",
    "contexts = []\n",
    "retrieve_time = []\n",
    "completion_time = []\n",
    "\n",
    "questions = questions[:2]\n",
    "\n",
    "init_openai()\n",
    "for i, query in enumerate(questions):\n",
    "    print(i, \"begins\")\n",
    "    t0 = time.time()\n",
    "    prompt, search_results = search_db(query, source)\n",
    "    t1 = time.time()\n",
    "    response = get_completion_openai(prompt, model=model)\n",
    "    t2 = time.time()\n",
    "    contexts.append(search_results) \n",
    "    answers.append(response)\n",
    "    retrieve_time.append(t1 - t0)\n",
    "    completion_time.append(t2 - t1)\n",
    "\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths,\n",
    "    \"retrieve_time\": retrieve_time,\n",
    "    \"completion_time\": completion_time, \n",
    "}\n",
    "dataset = Dataset.from_dict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
